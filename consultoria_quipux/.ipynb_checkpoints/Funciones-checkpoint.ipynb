{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones del Modelo\n",
    "\n",
    "En el modelo podemos diferenciar dos tipos de funciones:\n",
    "+ funciones para procesar información\n",
    "+ funciones de apoyo\n",
    "\n",
    "Generalmente las funciones para procesar informacion necesitan el objeto 'Indice' para guiar el procedimiento y la informacion de origen o destino de la data para las operaciones.  \n",
    "\n",
    "Su objetivo es poder aplicar diferentes métodos de análisis a la data, con diversas variables y conservar una trazabilidad de los datos que se generan, creando indicadores que nos permite filtrar o clasificar los registros.  \n",
    "\n",
    "Las funciones de apoyo son más sencillas y no requieren variables especificas, se caracterizan por generar informes, llevar una trazabilidad de las salidas, etc. Se deben desarrollar para optimizar la operación del modelo.\n",
    "\n",
    "\n",
    "## Métodos\n",
    "\n",
    "Los métodos son las funciones para procesar la informacién, se caracterizan por ser muy potentes y definir indicadores claros para clasificar o filtrar la información en caso que se utilice.\n",
    "\n",
    "A continuación veremos uno a uno los métodos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contenida\n",
    "\n",
    "La función se llama palabra_contenida_sin_orden(indice, origen, destino) y se trata de poder hayar similitudes entre dos cadenas de texto, dado que tengan una parte de la cadena incluida en la otra o que esten en desorden, que este separada por un espacio, de la siguiente forma:\n",
    "\n",
    "> Range Rover == Rover Ranger  \n",
    "> Aveo Sport == Aveo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.l1_contiene_l2 import l1_contiene_l2\n",
    "import utils.utils as ut\n",
    "\n",
    "def palabra_contenida_sin_orden(indice, origen, destino):    \n",
    "\n",
    "    ut.save_log(f'Tamaño matrices a comparar: origen {len(origen)}, destino {len(destino)}')\n",
    "        \n",
    "    ut.save_log('Calculando comparaciones')                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "\n",
    "    if len(indice.estrictas) > 0:\n",
    "        df = origen.merge(destino, how = 'inner', on = indice.estrictas, \n",
    "                suffixes = ('_origen','_destino'))\n",
    "            \n",
    "    else: \n",
    "        df = origen.copy()\n",
    "        df[indice.destino_id] = destino[indice.destino_id].copy()\n",
    "        df[indice.COL_1] = origen[indice.ON].copy()\n",
    "        df[indice.COL_2] = destino[indice.ON].copy()\n",
    "\n",
    "    ut.save_log(f'Número de comparaciones: {len(df)}')\n",
    "\n",
    "    df1 = pd.DataFrame()\n",
    "    df2 = pd.DataFrame()\n",
    "    \n",
    "    ###########################\n",
    "    ###########################\n",
    "\n",
    "#     baches para lineas qx\n",
    "\n",
    "    bach = df[indice.estrictas].drop_duplicates().copy()\n",
    "\n",
    "    for referencia, marca in bach.itertuples(index=False):\n",
    "\n",
    "        df_temp = df.loc[df['TABLA'].isin([referencia]) & df['marca'].isin([marca])]\n",
    "\n",
    "        df1 = pd.concat(\n",
    "               [df1, l1_contiene_l2(df_temp, indice.COL_1, indice.COL_2)])\n",
    "        df2 = pd.concat(\n",
    "               [df2, l1_contiene_l2(df_temp, indice.COL_2, indice.COL_1)])\n",
    "\n",
    "    #####################\n",
    "\n",
    "    ## Sin baches\n",
    "    \n",
    "#     df1 = pd.concat([df1, l1_contiene_l2(df, indice.COL_1, indice.COL_2)],\n",
    "#             axis=1)\n",
    "#     df2 = pd.concat([df2, l1_contiene_l2(df, indice.COL_2, indice.COL_1)],\n",
    "#             axis=1)\n",
    "\n",
    "    #####################\n",
    "    #####################\n",
    "    \n",
    "    # Concatenamos resultados y generamos la salida\n",
    "    \n",
    "    df = pd.concat([df1, df2])\n",
    "    df = df[indice.origen_id + indice.destino_id + [indice.COL_1, indice.COL_2] \n",
    "            + indice.estrictas + indice.conservar]\n",
    "    df.loc[:,'Palabras_contenida'] = 'SI'\n",
    "    sin_coincidencia = origen[~origen[indice.origen_id[0]].isin(\n",
    "            df[indice.origen_id[0]])]\n",
    "\n",
    "    ut.save_log(f'Resultado: homologaciones por palabras contenidas {len(df)}')\n",
    "    ut.save_log(f'Resultados sin coincidencia: {len(sin_coincidencia)}')        \n",
    "    \n",
    "    return df, sin_coincidencia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cruzar\n",
    "\n",
    "La funcion se llama destino_origen(indice, origen) y tiene como objetivo realizar automaticamente una verificacion cruzada de la informacion que se quiere procesar.  \n",
    "Toma una lista de archivos procesados por otros metodos y los verifica mediante el metodo de semejanza contra los datos de destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5b270bcb90d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetadir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetodos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msemejanza\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msemejanza\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'metadata'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import metadata.directory as metadir\n",
    "import re\n",
    "import pandas as pd\n",
    "import metodos.semejanza as semejanza\n",
    "import utils.utils as ut\n",
    "\n",
    "\n",
    "def destino_origen(indice, origen):\n",
    "\n",
    "    ut.save_log('### Calculando cruces en base de datos') \n",
    "    # Lectura nombres de archivos a trabajar\n",
    "\n",
    "    dic_archivos = open(file=os.path.abspath(os.path.join(metadir.Path.CARPETA_RESULTADOS, 'dic_archivos.txt')), mode='r')\n",
    "     \n",
    "    # En listamos y depuracmos los nombres de los archivos\n",
    "\n",
    "    list_files = dic_archivos.readlines()\n",
    "    list_files_edit = [i.replace('\\n', '') for i in list_files]\n",
    "\n",
    "    # Leemos los archivos y los trabajamos\n",
    "    \n",
    "    for archivo in list_files_edit:\n",
    "        ut.save_log(f'Lectura de datos de archivo: {archivo}') \n",
    "        df = pd.read_csv(os.path.abspath(os.path.join(metadir.Path.CARPETA_RESULTADOS, archivo)), sep = '\\t', dtype=object) \n",
    "        \n",
    "        # ut.save_log(f'Tamaño matrices a comparar: origen {len(origen)}, destino '\n",
    "        #     + f'{len(df)}')\n",
    "        #nombres_cols = input('')    \n",
    "\n",
    "        # Calculamos semajanza\n",
    "            \n",
    "        ut.save_log('### Calculando semejanza')\n",
    "        \n",
    "        df = df.drop( indice.destino_id, axis = 1)\n",
    "\n",
    "        temp_1 = indice.destino_id[0]\n",
    "        temp_2 = indice.origen_id[0]\n",
    "        indice.origen_id[0] = temp_1\n",
    "        indice.destino_id[0] = temp_2\n",
    "        homologacion, sin_coincidencia = semejanza.semejanza( indice, origen, df)\n",
    "        indice.origen_id[0] = temp_2\n",
    "        indice.destino_id[0] = temp_1\n",
    "\n",
    "        name = 'cruce_' + archivo\n",
    "        homologacion.to_csv(\n",
    "            os.path.abspath(os.path.join(metadir.Path.CARPETA_RESULTADOS, name)), \n",
    "            sep = '\\t', index = False)  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desagregar\n",
    "\n",
    "La funcion se llama desempate(indice, df, columna), especialmente disenada para Hologacion de vehiculos, se trata de identificar caracteristicas inconsistentes en una base de datos y poder retirarlas sin afectar previamente la data. Este proceso es un data clenaing predetermina especificamente para vehiculos pero se puede parametrizar para otros procesos.  \n",
    "Usamos expresiones regulares y archivos de referencia para generar las nuevas columnas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import metadata.directory as metadir\n",
    "\n",
    "def desagregar(indice, origen):  \n",
    "\n",
    "    print(f'Desagregamos la columna: {indice.ON} de la base de datos origen')\n",
    "\n",
    "    origen[indice.ON + '_ORIGINAL'] = origen[indice.ON]\n",
    "    \n",
    "    # Lecutra de datos con características a desagregar\n",
    "    \n",
    "    name = metadir.Path.limp_col\n",
    "    reglas = pd.read_csv(name, sep=';')\n",
    "    reglas = (reglas[reglas['campo'] != 'especial']\n",
    "        .assign(ref_len=reglas.referencia.str.len())\n",
    "        .sort_values('ref_len', ascending=False)\n",
    "        .drop(columns='ref_len'))\n",
    "    \n",
    "    # Corrección de nombres\n",
    "    \n",
    "    reglas['campo'] = reglas['campo'].str.replace('CAJA', 'TIPO_CAJA')\n",
    "    reglas['campo'] = reglas['campo'].str.replace('CARROCERIA', 'sin categoria')\n",
    "    reglas['campo'] = reglas['campo'].str.replace('CAPACIDAD', 'sin categoria')\n",
    "    reglas['campo'] = reglas['campo'].str.replace('PUERTAS', 'PUERTAS_LINEA')\n",
    "    reglas['campo'] = reglas['campo'].str.replace('CLASE', 'sin categoria')\n",
    "    reglas['campo'] = reglas['campo'].str.replace('CILINDRAJE', 'CILINDRAJE_LINEA')\n",
    "    \n",
    "    # Reemplazo por tipo de campo\n",
    "    for i in range(reglas.shape[0]):\n",
    "\n",
    "        encuentra, reemplaza, columna = reglas.iloc[i, 1:4].values\n",
    "\n",
    "        if columna != 'sin categoria' and not columna in origen.columns:\n",
    "            print('revisar columna inexistente: ' + columna)\n",
    "            import pdb; pdb.set_trace()     \n",
    "\n",
    "        index = origen[origen[indice.ON].str.contains(encuentra.upper(), \n",
    "                regex=False, na=False)].index\n",
    "\n",
    "        if columna != 'sin categoria':\n",
    "            origen.loc[index, columna] = reemplaza\n",
    "\n",
    "        origen.loc[index, indice.ON] = (\n",
    "                origen.loc[index, indice.ON].str.replace(\n",
    "                        '(^| |-)' + re.escape(encuentra) + '($| |-)', ' '))      \n",
    "\n",
    "    origen[indice.ON] = origen[indice.ON].str.strip()\n",
    "\n",
    "    return origen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desempate\n",
    "\n",
    "La funcion se llama desempate(indice, df, columna)\n",
    "\n",
    "Se debe tener reglas claras, basadas en operaciones exactas como por ejemplo min, max, o casos solucionados en 'tablas de verdad'.\n",
    "\n",
    "Funciona para establecer registros unicos o mas especificos de acuerdo a variables adicionales de comparacion, sin necesidad de entrar a discutir la veracidad de esas varibales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import utils.utils as ut\n",
    "\n",
    "import metadata.directory as metadir\n",
    "\n",
    "CARPETA_DATA = metadir.Path.CARPETA_DATA\n",
    "\n",
    "def desempate(indice, df, columna):\n",
    "\n",
    "    # Filtramos homologaciones únicas     \n",
    "\n",
    "    duplicadas = df[df.duplicated('ID_UNICO', keep = False)]\n",
    "    \n",
    "    total = df.copy()\n",
    "    \n",
    "    unicos = df.drop_duplicates('ID_UNICO', keep = False )\n",
    "    \n",
    "    ut.save_log('Criterio de trabajo: ', columna)\n",
    "\n",
    "    ut.save_log(f'Registros unicos: {len(unicos)}, Registros duplicados: {len(duplicados)}')    \n",
    "    \n",
    "    if columna == 'CILINDRAJE':\n",
    "        duplicadas['_diff'] = np.abs(duplicadas.CILINDRAJE_destino - duplicadas.CILINDRAJE_origen) # Cilindraje Destino menos Origen\n",
    "        duplicadas = duplicadas[duplicadas._diff == duplicadas.groupby('ID_UNICO')['_diff'].transform(min)]\n",
    "        duplicadas = duplicadas.drop(columns=['_diff'])\n",
    "\n",
    "    elif columna == 'CAPACIDAD':\n",
    "        duplicadas['_diff'] = np.abs(duplicadas.Capacidad_destino - duplicadas.Capacidad_origen)\n",
    "        duplicadas = duplicadas[duplicadas._diff == duplicadas.groupby('ID_UNICO')['_diff'].transform(min)]\n",
    "        duplicadas = duplicadas.drop(columns=['_diff'])\n",
    "\n",
    "    elif columna == 'PROMEDIO':\n",
    "        duplicadas = duplicadas[\n",
    "            duplicadas.Promedio == duplicadas.groupby('ID_UNICO')['Promedio'].transform(max)\n",
    "        ].drop_duplicates('ID_UNICO')\n",
    "\n",
    "    else:            \n",
    "    # Programar solución        \n",
    "        \n",
    "        if columna == 'CAJA':\n",
    "            criterios = pd.read_csv(os.path.abspath(os.path.join(CARPETA_DATA, 'criterio_cajas.csv',)), \n",
    "                                    sep = ';', dtype = object,engine='python')\n",
    "            \n",
    "        elif columna == 'NRO_PUERTAS':\n",
    "            criterios = pd.read_csv(os.path.abspath(os.path.join(CARPETA_DATA, 'criterio_puertas.csv',)), \n",
    "                                    sep = ';', dtype = object, engine='python')\n",
    "            criterios.rename(index = str, columns = {criterios.columns[0] :'VALOR'}, inplace = True)\n",
    "        elif columna == 'TRACCION':\n",
    "            criterios = pd.read_csv(os.path.abspath(os.path.join(CARPETA_DATA, 'criterio_traccion.csv',)), \n",
    "                                    sep = ';', dtype = object, engine='python')\n",
    "            criterios.rename(index = str, columns = {criterios.columns[0] :'VALOR'}, inplace = True)\n",
    "             \n",
    "        values = criterios['VALOR'].to_frame()\n",
    "        values['BIN'] = [2**(i) for i in range(len(values))]\n",
    "\n",
    "        for label, valor in values.itertuples(index=False):\n",
    "            duplicadas.loc[duplicadas[columna + '_destino'] == label, 'suma'] = valor\n",
    "        \n",
    "        duplicadas.suma = duplicadas.suma.astype(str)\n",
    "        duplicadas.suma = duplicadas.suma.str.replace('.0', '')\n",
    "        duplicadas.suma = duplicadas.suma.astype(int)\n",
    "\n",
    "        criterios_matriz = pd.melt(criterios, id_vars=['VALOR'], value_vars=criterios.columns[1:], \n",
    "                                              var_name='suma', value_name='criterio')\n",
    "\n",
    "        criterios_matriz['suma'] = criterios_matriz['suma'].astype(float)\n",
    "\n",
    "        # uniques = duplicadas[['ID_UNICO', 'suma']].groupby(['ID_UNICO', 'suma']).suma.unique()\n",
    "        \n",
    "        # suma = pd.DataFrame({'suma': np.nansum(pd.DataFrame(uniques.values.tolist()), axis=1)}, index=uniques.index)\n",
    "\n",
    "        suma = duplicadas[indice.origen_id + ['suma']].drop_duplicates().groupby(indice.origen_id).sum().reset_index()\n",
    "\n",
    "        duplicadas = duplicadas.drop(columns=['suma'])\n",
    "        \n",
    "        duplicadas = duplicadas.merge(suma, on = indice.origen_id)\n",
    "        \n",
    "        duplicadas = duplicadas.merge(criterios_matriz, left_on=['suma', columna + '_origen'], \n",
    "                right_on=['suma', 'VALOR'])  \n",
    "        duplicadas_ = duplicadas[(duplicadas.criterio != '>') & (duplicadas[columna + '_destino'] == duplicadas.criterio)]\n",
    "        duplicadas = duplicadas[duplicadas.criterio == '>']\n",
    "        duplicadas = pd.concat([duplicadas, duplicadas_], sort = False)\n",
    "        duplicadas = duplicadas.drop(columns=['suma', 'VALOR', 'criterio'])\n",
    "\n",
    "    \n",
    "    ut.save_log(f'Registros duplicados despues de criterio de desempate: {len(duplicados)}')\n",
    "\n",
    "    df_consolidado = pd.concat([duplicadas, unicos], sort=False)\n",
    "\n",
    "    # sin_solucion = total[total[indice.origen_id].isin(df[indice.origen_id])]\n",
    "    \n",
    "    return df_consolidado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exactas\n",
    "\n",
    "La funcion se llama exactas(indice, origen, destino, k)\n",
    "\n",
    "Es la automatizacion de varios procesos:\n",
    "1. Calculo de posibles combinaciones de variables\n",
    "2. Hallar registros con coincidencias exactas en todos los campos\n",
    "3. Hallar registros con coincidencias exactas en campos especificos\n",
    "4. Filtar y consolidar los registros de la base de datos de origen\n",
    "\n",
    "Son procesos muy simples pero en esta configuracion optimiza mucho el analisis y garantiza la integridad de la informacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import numpy as np\n",
    "import os\n",
    "\n",
    "import metadata.directory as metadir\n",
    "import utils.utils as ut\n",
    "\n",
    "path = metadir.Path.CARPETA_RESULTADOS\n",
    "\n",
    "def exactas(indice, origen, destino, k):\n",
    " \n",
    "    print(f'Combinaciones a comparar: {len(indice.on_cols)}')\n",
    "\n",
    "    for i, comb in enumerate(indice.on_cols):\n",
    "        print(f'Combinación {i+1}: {comb}') \n",
    "\n",
    "    # Aquí podemos editar cualquier comparación no deseada\n",
    "    print('¿Desea eliminar alguna combinación? pruebe eliminando una '\n",
    "            + 'combinación a la vez.')\n",
    "    print('Si desea continuar digite el número cero(0)')\n",
    "    var = int(input(\n",
    "            f'Escoge un número entero entre 1 y {len(indice.on_cols)}: '))\n",
    "    print(f'Digitó el número: {var}')\n",
    "\n",
    "    while var != 0:\n",
    "\n",
    "        del indice.on_cols[var-1]\n",
    "        \n",
    "        print(f'Combinaciones a comparar: {len(indice.on_cols)}')\n",
    "\n",
    "        for i, comb in enumerate(indice.on_cols):\n",
    "            print(f'Combinación {i+1}: {comb}')\n",
    "        \n",
    "        print('¿Desea eliminar alguna combinación? pruebe insertando '\n",
    "                + 'uno o más dígitos separados por comas.')\n",
    "        print('para continuar digite el número cero(0)')\n",
    "        var = int(input(\n",
    "                f'Escoge uno o más números enteroS entre 1 y '\n",
    "                + f'{len(indice.on_cols)}: '))\n",
    "        print(f'Digitó el número: {var}')\n",
    "\n",
    "    # Editar orden de las comparaciones\n",
    "\n",
    "    print('¿Desea reorganizar las combinaciones? pruebe organizando una '\n",
    "            + 'combinación a la vez.')\n",
    "    print('Si desea continuar digite el número cero(0)')\n",
    "    var_2 = int(input(f'Escoge la combinación: '))\n",
    "    print(f'Digitó el número: {var_2}')\n",
    "    \n",
    "\n",
    "    while var_2 != 0:\n",
    "        \n",
    "        indx = int(input(f'Escoge la posición que deseas que ocupe: '))\n",
    "        print(f'Digitó el número: {indx}')\n",
    "        item = indice.on_cols.pop(var_2-1)\n",
    "        indice.on_cols.insert(indx-1, item)\n",
    "\n",
    "        print(f'Combinaciones a comparar: {len(indice.on_cols)}')\n",
    "\n",
    "        for i, comb in enumerate(indice.on_cols):\n",
    "            print(f'Combinación {i+1}: {comb}')\n",
    "\n",
    "        print('¿Desea reorganizar las combinaciones? pruebe organizando '\n",
    "                + 'una combinación a la vez.')\n",
    "        print('Si desea continuar digite el número cero(0)')\n",
    "        var_2 = int(input(f'Escoge la combinación: '))\n",
    "        print(f'Digitó el número: {var_2}')    \n",
    "\n",
    "    ut.save_log(f'Combinaciones a comparar: {len(indice.on_cols)}')\n",
    "\n",
    "    for i, comb in enumerate(indice.on_cols):\n",
    "        ut.save_log(f'Combinación {i+1}: {comb}')\n",
    "\n",
    "    ut.save_log(f'Tamaño matrices a comparar: origen {len(origen)}, destino '\n",
    "            + f'{len(destino)}')\n",
    "\n",
    "    no_match = origen.copy()\n",
    " \n",
    "    for j, i in enumerate(indice.on_cols):\n",
    "        \n",
    "        if j == 0:\n",
    "                exactas = pd.merge( no_match[indice.origen_cols], \n",
    "                                        destino[indice.destino_cols], \n",
    "                                        on = i)  \n",
    "\n",
    "                #exactas = exactas.drop_duplicates(indice.destino_id)\n",
    "                no_match = no_match[~no_match[indice.origen_cols[0]].isin(\n",
    "                        exactas[indice.origen_cols[0]])]\n",
    "                ut.save_log(f'Coincidentes exactas: {len(exactas)}, restantes: '\n",
    "                        + f'{len(no_match)}')\n",
    "                          \n",
    "        else:\n",
    "                exactas_incompletas = pd.merge( no_match[indice.origen_cols], \n",
    "                                        destino[indice.destino_cols], \n",
    "                                        on = i, suffixes= ('', '_modelo'))                 \n",
    "                \n",
    "                no_match = no_match[~no_match[indice.origen_cols[0]].isin(\n",
    "                        exactas_incompletas[indice.origen_cols[0]])]\n",
    "                ut.save_log(f'Columnas a comparar {i}, resultado: '\n",
    "                        + f'{len(exactas_incompletas)} similitudes, restantes: '\n",
    "                        + f'{len(no_match)}')\n",
    "                name = 'detalle_comparacion_' + str(i) + '_' + str(k) + '.csv'\n",
    "                exactas_incompletas.to_csv(\n",
    "                        os.path.abspath(os.path.join(path, name)), sep = ';', \n",
    "                        index = False)\n",
    "                ut.save_log(f'Archivos generado: detalle_comparacion {j}')\n",
    "                ut.docs_save(name)\n",
    "\n",
    "                del exactas_incompletas\n",
    "\n",
    "        \n",
    "    return exactas, no_match\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semejanza\n",
    "\n",
    "La funcion se llama semejanza(indice, origen, destino)\n",
    "\n",
    "Consiste en generar un filtro a partir de un indicador de distancia entre dos cadenas de texto de la siguiente manera:\n",
    "\n",
    "> c1 = Rochelle  \n",
    "  c2 = Rochele  \n",
    "  dist(c1,c2) = 0.9   \n",
    "  si x > 0.7 -----> C > x == OK y C < x == NOT OK\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import ratio\n",
    "from Levenshtein import jaro_winkler\n",
    "import utils.utils as ut\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def semejanza(indice, origen, destino):\n",
    "\n",
    "    ut.save_log(f'Tamaño matrices a comparar: origen {len(origen)}, destino {len(destino)}')\n",
    "        \n",
    "    ut.save_log('Calculando comparaciones') \n",
    "\n",
    "    # Aquí hallamos todas las posibles comparaciones\n",
    "\n",
    "    if len(indice.estrictas) > 0:\n",
    "        df = origen.merge(destino, how = 'left', on = indice.estrictas, suffixes = ('_origen','_destino'))\n",
    "            \n",
    "    else: \n",
    "        df = origen.copy()\n",
    "        df[indice.destino_id] = destino[indice.destino_id].copy()\n",
    "        df[indice.COL_1] = origen[indice.ON].copy()\n",
    "        df[indice.COL_2] = destino[indice.ON].copy()\n",
    "    \n",
    "    ut.save_log(f'Número de comparaciones: {len(df)}')\n",
    "    ut.save_log('Cálculo de distancia levenshtein')\n",
    "\n",
    "    ## Aplicar Group by MARCA, referencia y LINEA para reducir tiempos. \n",
    "    bach = df[indice.estrictas].drop_duplicates().copy()\n",
    "    \n",
    "    print(f'Número de baches: {bach.shape[0]}')\n",
    "\n",
    "    porcentaje = float(input(\n",
    "            f'Digite porcentaje para filtrar los resultados: '))\n",
    "    ut.save_log(f'Porcentaje para filtrar resultados: {int(porcentaje*100)}%')\n",
    "    \n",
    "    count = 0\n",
    "    df_consolidado = None\n",
    "\n",
    "    for tabla, marca in bach.itertuples(index=False):\n",
    "        \n",
    "        count = count + 1\n",
    "        #print(f'Bache: # {count} de {bach.shape[0]} avance {round((count/bach.shape[0])*100,2)}%')\n",
    "\n",
    "        df_temp = df.loc[df['TABLA'].isin([tabla]) & df['marca'].isin([marca])].copy()\n",
    "        \n",
    "        df_temp['levenshtein'] = df_temp.apply(lambda x: ratio(str(x[indice.COL_1]), \n",
    "                str(x[indice.COL_2])), axis=1)\n",
    "\n",
    "        df_temp = df_temp[df_temp.levenshtein >= porcentaje]\n",
    "\n",
    "        if count == 0:            \n",
    "            df_consolidado = df_temp.copy()\n",
    "        else:\n",
    "            df_consolidado = pd.concat([df_consolidado, df_temp])\n",
    "        \n",
    "    print(f'Bache: # {count} de {bach.shape[0]} avance {round((count/bach.shape[0])*100,2)}%')\n",
    "#     ut.save_log('Cálculo de distancia jaro-winkler')\n",
    "#     df['jaro-winkler'] = df.apply(lambda x: jaro_winkler(str(x[indice.COL_1]), \n",
    "#             str(x[indice.COL_2]), 0.1), axis=1)\n",
    "    \n",
    "    ut.save_log(f'Resultado: Mayores al {int(porcentaje*100)}% = {len(df_consolidado)}')\n",
    "    ut.save_log('*ATENCIÓN: puede que aún falten filtros para establecer '\n",
    "            + 'la mejor coincidencia.')\n",
    "    \n",
    "    df_consolidado = df_consolidado[indice.origen_id + indice.destino_id + [indice.COL_1, indice.COL_2] \n",
    "            + indice.estrictas + ['levenshtein'] \n",
    "            + indice.conservar]\n",
    "    sin_coincidencia = origen[~origen[indice.origen_id[0]].isin(\n",
    "        df_consolidado[indice.origen_id[0]])]\n",
    "\n",
    "    ut.save_log(f'Resultados sin coincidencia: {len(sin_coincidencia)}')\n",
    "    \n",
    "    return df_consolidado, sin_coincidencia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de apoyo\n",
    "\n",
    "Generalmente son funciones de apoyo o generacion de documentos, se puede optimizar el modelo con diferentes funciones de apoyo sin necesidad de cambiar los metedos de analisis de informacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar informe\n",
    "\n",
    "Su objetivo es tomar las salidas del modelos y los resultados de los metodos del modelo y generar una salida en formato HTML o ipyn como bitacora del modelo.\n",
    "\n",
    "Este resultado se guarda en la carpeta de resultados del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat as nbf\n",
    "import io\n",
    "import os\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "#from objetos.datos import *\n",
    "import metadata.directory as metadir\n",
    "\n",
    "import datetime\n",
    "\n",
    "path = metadir.Path.CARPETA_RESULTADOS\n",
    "\n",
    "def generar_informe():\n",
    "    \n",
    "    nb = nbf.v4.new_notebook()\n",
    "    nb['cells'] = [nbf.v4.new_markdown_cell(\"\"\"\\\n",
    "# Bitácora Modelo\"\"\")]\n",
    "\n",
    "    # Para incertar texto antes del reporte de resultados\n",
    "    fecha_y_hora  =  datetime.datetime.now() \n",
    "    \n",
    "    texto = 'fecha: ' + fecha_y_hora.strftime('%d/%m/%Y %H:%M:%S')\n",
    "    nb['cells'].append(nbf.v4.new_markdown_cell(texto))\n",
    "\n",
    "    fp = io.open(os.path.abspath(os.path.join(path, 'logs.txt')), mode='r', \n",
    "            encoding='utf-8')\n",
    "\n",
    "    for i, line in enumerate(fp):\n",
    "        if i == 0:\n",
    "            nb['cells'].append(nbf.v4.new_markdown_cell(str(line)[1:]))\n",
    "        else:\n",
    "            nb['cells'].append(nbf.v4.new_markdown_cell(str(line)))\n",
    "    fp.close()\n",
    "\n",
    "    nbf.write(nb, os.path.abspath(os.path.join(path, 'bitacora_modelo.ipynb')))\n",
    "    check_output('python -m nbconvert --to=html '\n",
    "            + '--TemplateExporter.exclude_input=True ' \n",
    "            + os.path.abspath(os.path.join(path, 'bitacora_modelo.ipynb')), \n",
    "            shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### l1_contiene_l2\n",
    "\n",
    "Una funcion especifica para el metodo de palabras cotenidas que nos permite desagregar una cadena de texto separandola por espacios en blanco y calculando cuantas palabras coinciden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def l1_contiene_l2(df, columna1, columna2):\n",
    "    \n",
    "    ls = df.loc[:, [columna1, columna2]\n",
    "            ].drop_duplicates([columna1, columna2]\n",
    "            ).reset_index(drop=True)\n",
    "\n",
    "    linea1_temp = ls[columna1].str.split(' ', expand=True)\n",
    "\n",
    "    linea1_split = pd.melt(\n",
    "            pd.concat([ls, linea1_temp], axis=1), \n",
    "            id_vars=[columna1, columna2],\n",
    "            value_vars=linea1_temp.columns,\n",
    "            var_name='posicion',\n",
    "            value_name='palabra'\n",
    "            ).drop_duplicates([columna1, columna2, 'palabra']).reset_index(\n",
    "            drop=True).dropna()\n",
    "\n",
    "    linea1_split['n_words'] = linea1_split.groupby([columna1, columna2]\n",
    "            ).palabra.transform('count')\n",
    "\n",
    "    linea2_temp = ls[columna2].str.split(' ', expand=True)\n",
    "\n",
    "    linea2_split = pd.melt(\n",
    "            pd.concat([ls, linea2_temp], axis=1), \n",
    "            id_vars=[columna1, columna2],\n",
    "            value_vars=linea2_temp.columns,\n",
    "            var_name='posicion',\n",
    "            value_name='palabra'\n",
    "            ).drop_duplicates([columna1, columna2, 'palabra']).reset_index(\n",
    "            drop=True).dropna()\n",
    "\n",
    "    if linea1_split.shape[0] == 0 or linea1_split.shape[0] == 0:\n",
    "        return None   \n",
    "\n",
    "    lineas = linea1_split.merge(\n",
    "            linea2_split,\n",
    "            how='inner',\n",
    "            on=[columna1, columna2, 'palabra']\n",
    "            )\n",
    "\n",
    "    lineas['n_words_2'] = lineas.groupby([columna1, columna2]\n",
    "            ).palabra.transform('count')\n",
    "\n",
    "    lineas['resultado'] = lineas.n_words == lineas.n_words_2\n",
    "    lins_g = lineas.groupby([columna1, columna2]).resultado.all().reset_index()\n",
    "    df = df.merge(lins_g, on=[columna1, columna2], how='left')\n",
    "    df = df[df.resultado == True].drop(columns=['resultado'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set_up\n",
    "\n",
    "Descomprime la carpeta de entrada de datos y comprime la carpeta de resultados del modelo para evitar problemas de memoria.  \n",
    "\n",
    "Genera las condiciones necesarias para correr el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import metadata.directory as metadir\n",
    "import os\n",
    "\n",
    "\n",
    "def inicio():\n",
    "                 \n",
    "    fecha_ejecucion = datetime.datetime.now()\n",
    "\n",
    "    # Genera directorios Input Output\n",
    "    metadir.Path.generar_rutas_Input_Output(Input_ruta=metadir.Path.CARPETA_DATA, \n",
    "            Output_ruta=metadir.Path.CARPETA_RESULTADOS,firma='2019-05-17')\n",
    "\n",
    "    # Crear logs.txt en CARPETA_RESULTADOS\n",
    "    logs = open(file=os.path.abspath(os.path.join(metadir.Path.CARPETA_RESULTADOS, \n",
    "            'logs.txt')), mode='w') \n",
    "    logs.close()\n",
    "\n",
    "    dic_archivos = open(file=os.path.abspath(os.path.join(metadir.Path.CARPETA_RESULTADOS, \n",
    "            'dic_archivos.txt')), mode='w') \n",
    "    dic_archivos.close()\n",
    "\n",
    "def final():\n",
    "    fecha_ejecucion = datetime.datetime.now()\n",
    "    #Genera directorios Input Output\n",
    "    metadir.Path.comprimir_rutas_Input_Output_en_zip(\n",
    "            Input_ruta=metadir.Path.CARPETA_DATA, \n",
    "            Output_ruta=metadir.Path.CARPETA_RESULTADOS,\n",
    "            firma=fecha_ejecucion.strftime('%Y-%m-%d_%H%M%S')\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils\n",
    "\n",
    "Contiene las funciones mas pequenas y aleatorias:\n",
    "\n",
    "* La funcion cols_combinations(flexibles, estrictas) nos permite hallar todas las posibles combinaciones en una lista de variables\n",
    "\n",
    "* La funcion save_log(log) imprime y guarda las salidas programadas de cada metodo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it \n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#import re\n",
    "import os\n",
    "\n",
    "import metadata.directory as metadir\n",
    "\n",
    "# Obtenemos las combinaciones para aplicar simplificación\n",
    "\n",
    "def cols_combinations(flexibles, estrictas):\n",
    "    cols_comb = []\n",
    "    cols = flexibles + estrictas\n",
    "    for i in range(1,len(cols)+1):\n",
    "        comb = it.combinations(cols, i)\n",
    "        for i in comb:\n",
    "            cols_comb.append(i)\n",
    "    \n",
    "    # cols_comb de lista de tupla a lista de listas\n",
    "    un_tuple = []\n",
    "    for i in cols_comb:\n",
    "        y = [j for j in i] #+ estrictas\n",
    "        un_tuple.append(y)  \n",
    "    \n",
    "    return un_tuple \n",
    "\n",
    "# Función para registrar las salidas del modelo\n",
    "\n",
    "def save_log(log):\n",
    "    path = metadir.Path.CARPETA_RESULTADOS\n",
    "    print(log)\n",
    "#    with open(str(path / 'logs.txt'), 'a', encoding='utf8') as f:\n",
    "    with open(os.path.abspath(os.path.join(path, 'logs.txt')), 'a', \n",
    "              encoding='utf8') as f:\n",
    "        f.write(log + '\\n')\n",
    "\n",
    "def docs_save(name):\n",
    "    path = metadir.Path.CARPETA_RESULTADOS\n",
    "    #print(log)\n",
    "#    with open(str(path / 'logs.txt'), 'a', encoding='utf8') as f:\n",
    "    with open(os.path.abspath(os.path.join(path, 'dic_archivos.txt')), 'a', \n",
    "              encoding='utf8') as f:\n",
    "        f.write(name + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
